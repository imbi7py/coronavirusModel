{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for Training BERT model for Question/Answer\n",
    "- about confirmed cases by cities or province\n",
    "\n",
    "\n",
    "Create list of dictionnaries  \n",
    "One dict by url  \n",
    "For each url, dict keys are   'url', 'context', 'qas'  \n",
    "Each qas is a list of dict with key values are :  \n",
    "'id', 'is_impossible', 'question', 'answers'  \n",
    "Each answers of qas dicts are list of dict with key : 'text', 'answer_start'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# Import\n",
    "# built)in libs\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import json \n",
    "import copy\n",
    "import datetime\n",
    "# third-party libs\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TABLES_KCDC_UPDATES = os.getcwd() + '/tables_kcdc_updates.json'\n",
    "\n",
    "PATH_QA_KCDC = os.getcwd() + '/train_data_qa_kcdc.json'\n",
    "\n",
    "LIST_AREA = [\"Seoul\",\n",
    "\"Busan\",\n",
    "\"Daegu\",\n",
    "\"Incheon\",\n",
    "\"Gwangju\",\n",
    "\"Daejeon\",\n",
    "\"Ulsan\",\n",
    "\"Sejong\",\n",
    "\"Gyeonggi\",\n",
    "\"Gangwon\",\n",
    "\"Chungbuk\",\n",
    "\"Chungnam\",\n",
    "\"Jeonbuk\",\n",
    "\"Jeonnam\",\n",
    "\"Gyeongbuk\",\n",
    "\"Gyeongnam\",\n",
    "\"Jeju\"]\n",
    "\n",
    "DICT_AREA_SPECIAL_2 ={'Gyeonggi': [\"Gyeong\", \"gi\"],\n",
    "                    'Gangwon':[\"Gang\", \"won\"],\n",
    "                    'Chungbuk':[\"Chung\", \"buk\"],\n",
    "                    'Chungnam':[\"Chung\", \"nam\"],\n",
    "                    'Jeonbuk': [\"Jeon\", \"buk\"],\n",
    "                    'Jeonnam': [\"Jeon\", \"nam\"],\n",
    "                    'Gyeongbuk': [\"Gyeong\", \"buk\"],\n",
    "                    'Gyeongnam': [\"Gyeong\", \"nam\"],\n",
    "                    'Daejeon' : [\"Dae\", \"jeon\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file(path_file_name):\n",
    "    '''\n",
    "    Clean file already traited : rename file with date\n",
    "    '''\n",
    "    d = datetime.datetime.now()\n",
    "    str_date = '_' + d.strftime(\"%Y%m%d_%H_%M_%S\")\n",
    "       \n",
    "    res_re = re.search('\\.\\w+$', path_file_name)\n",
    "        \n",
    "    path_file_name_saved = \\\n",
    "        path_file_name[0:res_re.start()] + str_date + res_re.group(0)\n",
    "    if os.path.isfile(path_file_name):    \n",
    "        shutil.move(path_file_name, path_file_name_saved) \n",
    "        print('File {} moved!'.format(path_file_name_saved))\n",
    "    else:\n",
    "        print('File {} does not exist!'.format(path_file_name))\n",
    "    \n",
    "def create_search_pattern(area_curr, dict_special=DICT_AREA_SPECIAL_2,\n",
    "                         input_type=\"notags\"):\n",
    "    '''\n",
    "    Create pattern to search into text for all area\n",
    "    If special area, add special writting like for \"Jeonbuk\" : \n",
    "    \"Jeonbuk\" or \"Jeon-buk\" or \"Jeon-\\r\\n buk\"\n",
    "    '''\n",
    "    if area_curr in dict_special.keys():\n",
    "        \n",
    "        if input_type == \"notags\":\n",
    "            # patch if Gyeonggi seperate not in middle sometimes... \n",
    "            if area_curr == \"Gyeonggi\":\n",
    "                # (Gyeonggi)|(Gyeong-gi)|(Gyeong-{0,1}\\s{0,1}\\s{0,1}-{0,1}gi) \n",
    "                # (Gyeon-{0,1}\\s{0,1}\\s{0,1}-{0,1}ggi)\n",
    "                return '({})|({})|({})|({})'.format(area_curr,\n",
    "                    '-'.join(dict_special[area_curr]),\n",
    "                    '-{0,1}\\s{0,1}\\r\\n\\s{0,1}-{0,1}' \\\n",
    "                                        .join(dict_special[area_curr]), \n",
    "                    'Gyeon-{0,1}\\s{0,1}\\s{0,1}-{0,1}ggi')\n",
    "            \n",
    "            return '({})|({})|({})'.format(area_curr,\n",
    "                                  '-'.join(dict_special[area_curr]),\n",
    "            '-{0,1}\\s{0,1}\\r\\n\\s{0,1}-{0,1}'.join(dict_special[area_curr]))\n",
    "        else:\n",
    "            #'({}</span>)'.format('</span>.+>-'.join(DICT_AREA_SPECIAL_2[area_curr]))\n",
    "            return '({})|({})|({}</span>)'.format(area_curr,\n",
    "                        '-'.join(dict_special[area_curr]),\n",
    "                        '-{0,1}</span>.+>-{0,1}'.join(dict_special[area_curr]))            \n",
    "    else:\n",
    "        return area_curr\n",
    "    \n",
    "def find_start_num(context, area_curr, val_curr, index=None, \n",
    "                   input_type=\"notags\"):\n",
    "    '''\n",
    "    Find string number which is located at closest location from area_curr\n",
    "    '''\n",
    "    try:\n",
    "        # search every pattern into context long string like : \n",
    "        #    \"\\r\\n1,234 \" or \"\\r\\n1,234\\r\\n\" \n",
    "        # or \"\\r\\n1234 \" or  \"\\r\\n1234\\r\\n\" \n",
    "        \n",
    "        if input_type == \"notags\":\n",
    "            str_num_search = '((?<= )|(?<= \\r\\n))({:,d}|{})((?= )|(?=\\r\\n))'.\\\n",
    "                format(int(val_curr), int(val_curr))\n",
    "            len_first_mark = 0 # = len('\\r\\n')\n",
    "        else:\n",
    "            str_num_search = '>({:,d}|{})\\s*<'.format(int(val_curr), \n",
    "                                                          int(val_curr))\n",
    "            len_first_mark = 1 # len('>')            \n",
    "        \n",
    "        list_pos_num = []\n",
    "        for iter_find in re.finditer(str_num_search, context):\n",
    "            list_pos_num.append(iter_find.start())\n",
    "        arr_num = np.array(list_pos_num) \n",
    "        \n",
    "        # search every area_curr pattern into context string\n",
    "        # add to the search special area : \n",
    "        str_pat_area = create_search_pattern(area_curr, DICT_AREA_SPECIAL_2,\n",
    "                                           input_type) \n",
    "        list_pos_area = []\n",
    "        #print('str_pat_area : ', str_pat_area)\n",
    "        for iter_find in re.finditer(str_pat_area, context, re.DOTALL):\n",
    "            list_pos_area.append(iter_find.start())\n",
    "        arr_area = np.array(list_pos_area)\n",
    "\n",
    "        arr_delta = arr_num[:, np.newaxis] - arr_area\n",
    "        arr_delta[arr_delta<0] = 1e9\n",
    "        i_min = arr_delta.shape[0] - np.argmin(np.flip(np.amin(arr_delta, \n",
    "                                                               axis=1)))-1\n",
    "        delta_min = np.min(arr_delta)\n",
    "        start_curr = int(arr_num[i_min] + len_first_mark)\n",
    "        return delta_min, start_curr\n",
    "    except:\n",
    "        print(\"Error :[{}]:{} : {} \".format(index, area_curr, val_curr))\n",
    "        raise\n",
    "\n",
    "\n",
    "def keep_diff_in_list(list_text):\n",
    "    '''\n",
    "    Function to keep only different elements in list \n",
    "    (pops all elements repeted)\n",
    "    return index to keep\n",
    "    '''\n",
    "    list_text_ok = []\n",
    "    list_index = []\n",
    "    for index, answer_curr in enumerate(list_text):\n",
    "        list_text_comp = list_text.copy()\n",
    "        list_text_comp.remove(answer_curr)\n",
    "        if answer_curr not in list_text_comp:\n",
    "            list_text_ok.append(answer_curr)\n",
    "            list_index.append(index)\n",
    "            \n",
    "    return list_text_ok, list_index\n",
    "\n",
    "def filter_prepare_data_model_qa(list_qa):\n",
    "    '''\n",
    "    Test function for QA database for Q/A BERT model\n",
    "    \n",
    "    Check if values found are differents for each web pages \n",
    "    For each questions.\n",
    "    It is because il could have a problem to detect the first character if \n",
    "    several areas have the same number.\n",
    "    '''\n",
    "    list_qa_orig = copy.deepcopy(list_qa)\n",
    "    list_qa_ok = []\n",
    "    # check list_qa\n",
    "    for dict_curr in list_qa_orig:\n",
    "        context = dict_curr[\"context\"]\n",
    "        list_text = []\n",
    "        for q_curr in dict_curr['qas']:\n",
    "            list_text.append(q_curr[\"answers\"][0][\"text\"])\n",
    "        # if text answer is different from all others text answers, \n",
    "        # then it is possible to use this question\n",
    "        list_text_ok, list_index = keep_diff_in_list(list_text)\n",
    "        # if at least one element is unique\n",
    "        if len(list_index) > 0:\n",
    "            list_qas_curr = [dict_curr['qas'][i] for i in list_index]\n",
    "            dict_curr['qas'] = list_qas_curr\n",
    "            list_qa_ok.append(dict_curr)\n",
    "                \n",
    "    return list_qa_ok\n",
    "\n",
    "def prepare_data_model_qa(df_tables_kcdc_updates, path_json=PATH_QA_KCDC):\n",
    "    \"\"\"\n",
    "    Prepare JSON file for Question/Answering BERT model\n",
    "    \n",
    "    - Create list of dictionnaries\n",
    "    - One dict by url\n",
    "    - For each url, dict keys are   'url', 'context', 'qas'\n",
    "    - Each qas is a list of dict with key values are : \n",
    "        'id', 'is_impossible', 'question', 'answers'\n",
    "    - Each answers of qas dicts are list of dict with key : \n",
    "        'text', 'answer_start'\n",
    "    \"\"\"\n",
    "    # Creation of output\n",
    "    list_qa = []\n",
    "    list_delta_min = []\n",
    "    K_id = 0\n",
    "    # for each url, create a dictionnary\n",
    "    for index in df_tables_kcdc_updates.index:\n",
    "        #print(index)\n",
    "        # text data from webpage\n",
    "        context = df_tables_kcdc_updates.at[index, \"body\"]\n",
    "        # for each area, create a list of dictionnary for each question\n",
    "        list_qas = []\n",
    "        for area_curr in LIST_AREA:\n",
    "            val_curr = df_tables_kcdc_updates.at[index, area_curr]\n",
    "            if np.isnan(val_curr):\n",
    "                continue\n",
    "            \n",
    "            delta_min, start_curr = find_start_num(context, area_curr, val_curr, \n",
    "                                                   index, input_type=\"notags\")\n",
    "            # check if comma in text : \n",
    "            try:\n",
    "                #print(\"index {} ...\".format(index))\n",
    "                str_test = context[start_curr:start_curr + \\\n",
    "                               len('{}'.format(val_curr)) + 1]\n",
    "            except:\n",
    "                print(\"ERROR : \")\n",
    "                print(\"index: \", index)\n",
    "                print(\"area_curr: \", area_curr)\n",
    "                print(\"start_curr: \", start_curr)\n",
    "                raise Exception('ERROR!')\n",
    "                                \n",
    "            if re.search(\",\", str_test) != None:\n",
    "                text_val_curr = '{:,d}'.format(int(val_curr))\n",
    "            else:\n",
    "                text_val_curr = '{}'.format(int(val_curr))\n",
    "            \n",
    "            K_id += 1\n",
    "            qas_dict_curr = {'id': \"{:05}\".format(K_id),\n",
    "                            'is_impossible': False,\n",
    "                 \"question\": \"How many confirmed cases are in \" + \\\n",
    "                             area_curr + \"?\",\n",
    "                 'answers': [ \\\n",
    "                {'text': text_val_curr, \n",
    "                'answer_start': start_curr}]}\n",
    "            list_qas.append(qas_dict_curr)\n",
    "            list_delta_min.append(delta_min)\n",
    "\n",
    "\n",
    "        dict_curr = {'index':index, 'url': df_tables_kcdc_updates.at[index, \n",
    "                                                                     \"url\"],\n",
    "                    'context': context, \n",
    "                    'qas': list_qas}\n",
    "        if len(list_qas) == 0:\n",
    "            #print(\"index {} : list_qas empty!\".format(index))\n",
    "            continue\n",
    "        list_qa.append(dict_curr)\n",
    "    \n",
    "    # filter questions with unique answer by webpages\n",
    "    list_qa_ok = filter_prepare_data_model_qa(list_qa)\n",
    "\n",
    "    # Save as a JSON file\n",
    "    with open(path_json, 'w') as f:\n",
    "        json.dump(list_qa_ok, f)\n",
    "        \n",
    "    return list_delta_min, list_qa_ok\n",
    "\n",
    "def test_prepare_data_model_qa(list_qa):\n",
    "    '''\n",
    "    Test function for QA database for Q/A BERT model\n",
    "    \n",
    "    Check if text found in each web page is good : answer found in context.\n",
    "    For each questions.\n",
    "    '''\n",
    "    # check list_qa\n",
    "    for dict_curr in list_qa:\n",
    "        flag_ok = True\n",
    "        context = dict_curr[\"context\"]\n",
    "        for q_curr in dict_curr['qas']:\n",
    "            text_curr = q_curr[\"answers\"][0][\"text\"]\n",
    "            start_curr = q_curr[\"answers\"][0][\"answer_start\"]\n",
    "            text_found = context[start_curr:start_curr + len(text_curr)]\n",
    "            if text_found != text_curr:\n",
    "                flag_ok = False\n",
    "                print(\"ERROR idx:{} Q:{}\".format(dict_curr[\"index\"], \n",
    "                                                 q_curr[\"question\"]))\n",
    "                print(\"answer : \", text_curr)\n",
    "                print(\"found  : \", text_found)\n",
    "                print(dict_curr[\"url\"])\n",
    "\n",
    "    return flag_ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb. South Korea articles :  506\n"
     ]
    }
   ],
   "source": [
    "df_tables_kcdc_updates = pd.read_json(PATH_TABLES_KCDC_UPDATES)\n",
    "df_tables_kcdc_updates = \\\n",
    "    df_tables_kcdc_updates.sort_values(by=['date_published'])\n",
    "print(\"Nb. South Korea articles : \", df_tables_kcdc_updates.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /Users/gregory/Documents/CloudStationSinchon/Applications/python/CoronaVirus/code/coronavirusModel/train_data_qa_kcdc_20201113_10_35_28.json moved!\n"
     ]
    }
   ],
   "source": [
    "clean_file(PATH_QA_KCDC)\n",
    "list_delta_min, list_qa = prepare_data_model_qa(df_tables_kcdc_updates, \n",
    "                                                path_json=PATH_QA_KCDC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data processed : TEST OK ?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test data processed : TEST OK ?\")\n",
    "test_prepare_data_model_qa(list_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
